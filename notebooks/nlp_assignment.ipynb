{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tweet Sentiment Analysis\n",
    "\n",
    "### Projektziel\n",
    "Dieses Projekt zielt darauf ab, Tweets in positive und negative Kategorien zu klassifizieren.  \n",
    "Es verwendet moderne NLP-Techniken wie:\n",
    "- **TF-IDF**: Zur numerischen Darstellung von Textdaten.\n",
    "- **GloVe**: Vortrainierte Embeddings zur besseren Erfassung von Wortbeziehungen.\n",
    "- **Emoji2Vec**: Speziell zur Erkennung von Emoji-Bedeutungen.\n",
    "- **BERT**: Kontextuelle Embeddings für tiefere semantische Analysen.\n",
    "\n",
    "### Methodik\n",
    "1. **Datenaufbereitung**:\n",
    "   - Bereinigung der Tweets (z. B. Entfernung von URLs und Mentions).\n",
    "   - Tokenisierung, Stemming und Lemmatization.\n",
    "2. **Feature Engineering**:\n",
    "   - Nutzung von TF-IDF, GloVe, Emoji2Vec und BERT.\n",
    "   - Kombination der verschiedenen Features in einer skalierbaren Repräsentation.\n",
    "3. **Modelltraining**:\n",
    "   - Einsatz von LightGBM, einem leistungsstarken Algorithmus für tabellarische Daten.\n",
    "4. **Bewertung**:\n",
    "   - Analyse der Modellleistung mit Genauigkeit, F1-Score und Confusion-Matrix.\n",
    "\n",
    "### Datenquelle\n",
    "Die Daten stammen aus dem **NLTK Twitter-Datensatz**, der aus positiven und negativen Tweets besteht:\n",
    "- **Positive Tweets**: Glückliche oder optimistische Inhalte.\n",
    "- **Negative Tweets**: Kritische oder wütende Inhalte.\n",
    "\n",
    "### Ziel\n",
    "Erreichen einer Modellgenauigkeit von mindestens **90 %** auf den Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Korrigiere den Modulpfad, um zum `src`-Verzeichnis zu gelangen\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"src\"))  # Gehe ein Verzeichnis zurück und dann in \"src\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# Daten laden \n",
    "from dataset_preparation import load_twitter_data\n",
    "\n",
    "# Positive und negative Tweets laden\n",
    "positive_tweets, negative_tweets = load_twitter_data()\n",
    "\n",
    "# Überblick über die Daten\n",
    "print(f\"Anzahl positiver Tweets: {len(positive_tweets)}\")\n",
    "print(f\"Anzahl negativer Tweets: {len(negative_tweets)}\")\n",
    "\n",
    "# Beispiele anzeigen\n",
    "print(\"\\nBeispiel positiver Tweet:\")\n",
    "print(positive_tweets[0])\n",
    "print(\"\\nBeispiel negativer Tweet:\")\n",
    "print(negative_tweets[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importe und Ressourcen\n",
    "\n",
    "In diesem Schritt werden alle benötigten Bibliotheken und Ressourcen importiert.  \n",
    "Zusätzlich werden die vortrainierten Modelle (GloVe, Emoji2Vec und BERT) vorbereitet, um sie später in der Pipeline zu verwenden.\n",
    "\n",
    "- **NLTK**: Für Textverarbeitung wie Tokenisierung und Stoppwortentfernung.\n",
    "- **TF-IDF**: Vektorisierung des Texts.\n",
    "- **GloVe**: Vortrainierte Wort-Embeddings.\n",
    "- **Emoji2Vec**: Spezielle Vektoren für Emojis.\n",
    "- **BERT**: Kontextuelle Wort-Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Eigene Module importieren\n",
    "from dataset_preparation import load_twitter_data, split_data\n",
    "from preprocess import (\n",
    "    preprocess_tweet,\n",
    "    vectorize_with_tfidf,\n",
    "    load_glove_embeddings,\n",
    "    vectorize_with_glove,\n",
    "    load_emoji2vec,\n",
    "    vectorize_with_emojis,\n",
    "    load_bert_model,\n",
    "    vectorize_with_bert,\n",
    ")\n",
    "from train_model import train_lightgbm, evaluate_model, plot_confusion_matrix\n",
    "\n",
    "# Ressourcen für NLP vorbereiten\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Ressourcen initialisieren\n",
    "print(\"Alle Ressourcen und Bibliotheken erfolgreich importiert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing\n",
    "\n",
    "Das Preprocessing umfasst die Bereinigung und Tokenisierung der Tweets, gefolgt von der Anwendung von NLP-Techniken zur Erstellung von Features:\n",
    "\n",
    "1. **Bereinigung**:\n",
    "   - Entfernen von URLs, Mentions und Sonderzeichen.\n",
    "   - Tokenisierung der Texte in einzelne Wörter.\n",
    "\n",
    "2. **Linguistische Verarbeitung**:\n",
    "   - Entfernen von Stopwörtern (z. B. \"and\", \"the\").\n",
    "   - Lemmatisierung, um Wörter auf ihre Grundform zu reduzieren (z. B. \"running\" → \"run\").\n",
    "\n",
    "3. **Feature-Vektorisierung**:\n",
    "   - **TF-IDF**: Zuweisung von Gewichtungen basierend auf der Häufigkeit eines Wortes in einem Dokument und im gesamten Korpus.\n",
    "   - **GloVe**: Vortrainierte Wort-Embeddings, die semantische Beziehungen zwischen Wörtern berücksichtigen.\n",
    "   - **Emoji2Vec**: Spezielle Embeddings für Emojis.\n",
    "   - **BERT**: Kontextuelle Embeddings, die die Bedeutung eines Wortes basierend auf seinem Kontext erfassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten aufteilen\n",
    "from dataset_preparation import split_data\n",
    "\n",
    "print(\"Teile Daten in Training, Validierung und Test auf...\")\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(\n",
    "    positive_tweets, negative_tweets, balance=True\n",
    ")\n",
    "\n",
    "print(f\"Train Size: {len(X_train)}, Valid Size: {len(X_valid)}, Test Size: {len(X_test)}\")\n",
    "print(f\"Training: {sum(y_train)} positive, {len(y_train) - sum(y_train)} negative\")\n",
    "print(f\"Validation: {sum(y_valid)} positive, {len(y_valid) - sum(y_valid)} negative\")\n",
    "print(f\"Test: {sum(y_test)} positive, {len(y_test) - sum(y_test)} negative\")\n",
    "\n",
    "\n",
    "# Vorverarbeitung der Tweets\n",
    "print(\"Starte Preprocessing...\")\n",
    "\n",
    "# Bereinigung und Tokenisierung\n",
    "X_train_preprocessed = [preprocess_tweet(tweet) for tweet in X_train]\n",
    "X_valid_preprocessed = [preprocess_tweet(tweet) for tweet in X_valid]\n",
    "X_test_preprocessed = [preprocess_tweet(tweet) for tweet in X_test]\n",
    "\n",
    "print(\"Preprocessing abgeschlossen.\")\n",
    "\n",
    "# Beispiel-Tweets nach Preprocessing\n",
    "print(\"\\nBeispiel-Tweet vor und nach Preprocessing:\")\n",
    "print(f\"Original: {X_train[0]}\")\n",
    "print(f\"Preprocessed: {X_train_preprocessed[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Datenaufteilung\n",
    "\n",
    "In diesem Schritt werden die geladenen Tweets in drei Datensätze aufgeteilt:\n",
    "\n",
    "1. **Training**:\n",
    "   - Für das Training des Modells verwendet.\n",
    "2. **Validierung**:\n",
    "   - Für die Hyperparameter-Optimierung und zur Bewertung während des Trainings.\n",
    "3. **Test**:\n",
    "   - Für die abschließende Bewertung der Modellleistung.\n",
    "\n",
    "### **Stratifizierte Aufteilung**\n",
    "Die Aufteilung stellt sicher, dass die Klassenverteilung in allen Datensätzen konsistent bleibt (positive und negative Tweets sind gleichmäßig verteilt).\n",
    "\n",
    "### **Balance der Daten**\n",
    "Falls die Klassenverteilung unausgewogen ist, wird ein Oversampling angewandt, um sicherzustellen, dass beide Klassen im Trainingsdatensatz gleich stark vertreten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenaufteilung\n",
    "from dataset_preparation import load_twitter_data, split_data\n",
    "\n",
    "# Tweets laden\n",
    "positive_tweets, negative_tweets = load_twitter_data()\n",
    "\n",
    "# Aufteilen der Daten\n",
    "print(\"Teile Daten in Training, Validierung und Test auf...\")\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(\n",
    "    positive_tweets, negative_tweets, balance=True\n",
    ")\n",
    "\n",
    "# Überblick über die Datengröße\n",
    "print(f\"Train Size: {len(X_train)}, Valid Size: {len(X_valid)}, Test Size: {len(X_test)}\")\n",
    "print(f\"Training: {sum(y_train)} positive, {len(y_train) - sum(y_train)} negative\")\n",
    "print(f\"Validation: {sum(y_valid)} positive, {len(y_valid) - sum(y_valid)} negative\")\n",
    "print(f\"Test: {sum(y_test)} positive, {len(y_test) - sum(y_test)} negative\")\n",
    "\n",
    "# Beispiel eines Tweets und Labels\n",
    "print(\"\\nBeispiel-Tweets aus Trainingsdaten:\")\n",
    "print(f\"Tweet: {X_train[0]}, Label: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature-Engineering\n",
    "\n",
    "Im Feature-Engineering werden die Tweets in numerische Vektoren umgewandelt, die das Modell als Eingabe akzeptiert.  \n",
    "Dafür werden folgende Ansätze verwendet:\n",
    "\n",
    "1. **TF-IDF**:\n",
    "   - Erzeugt numerische Repräsentationen basierend auf der Häufigkeit von Wörtern.\n",
    "   - Erfasst Uni- und Bigramme mit einer maximalen Anzahl von 20.000 Features.\n",
    "\n",
    "2. **GloVe**:\n",
    "   - Verwendet vortrainierte Wort-Embeddings, die semantische Beziehungen zwischen Wörtern berücksichtigen.\n",
    "   - Tweets werden durch den Mittelwert der Embeddings aller enthaltenen Wörter dargestellt.\n",
    "\n",
    "3. **Emoji2Vec**:\n",
    "   - Spezialisierte Embeddings zur Erkennung von Bedeutungen von Emojis.\n",
    "   - Tweets werden durch den Mittelwert der Vektoren der enthaltenen Emojis dargestellt.\n",
    "\n",
    "4. **BERT**:\n",
    "   - Kontextuelle Embeddings, die die Bedeutung eines Wortes basierend auf seinem Kontext im Satz erfassen.\n",
    "   - Tweets werden durch den Mittelwert aller Tokens des Modells dargestellt.\n",
    "\n",
    "Alle erzeugten Features werden später kombiniert und skaliert, um sie für das Modelltraining vorzubereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Feature-Engineering vorbereiten\n",
    "from preprocess import (\n",
    "    vectorize_with_tfidf,\n",
    "    load_glove_embeddings,\n",
    "    vectorize_with_glove,\n",
    "    load_emoji2vec,\n",
    "    vectorize_with_emojis,\n",
    "    load_bert_model,\n",
    "    vectorize_with_bert,\n",
    ")\n",
    "\n",
    "# Prüfen, ob das Arbeitsverzeichnis bereits korrekt ist\n",
    "current_dir = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, \"./\"))\n",
    "\n",
    "if current_dir != project_dir:\n",
    "    os.chdir(project_dir)\n",
    "\n",
    "print(\"Aktuelles Arbeitsverzeichnis:\", os.getcwd())\n",
    "\n",
    "# Pfade zu GloVe und Emoji2Vec\n",
    "glove_path = \"./data/glove.twitter.27B.200d.txt\"\n",
    "emoji2vec_path = \"./data/emoji2vec.txt\"\n",
    "\n",
    "\n",
    "# GloVe-Embeddings laden\n",
    "print(\"Lade GloVe-Embeddings...\")\n",
    "glove_embeddings, glove_mean = load_glove_embeddings(glove_path)\n",
    "\n",
    "# Emoji2Vec-Embeddings laden\n",
    "print(\"Lade Emoji2Vec-Embeddings...\")\n",
    "emoji_vectors = load_emoji2vec(emoji2vec_path)\n",
    "\n",
    "# BERT-Modell laden\n",
    "print(\"Lade BERT-Modell...\")\n",
    "tokenizer, bert_model = load_bert_model(\"distilbert-base-uncased\")\n",
    "\n",
    "# TF-IDF-Vektorisierung\n",
    "print(\"Vektorisieren mit TF-IDF...\")\n",
    "tfidf_matrix_train, tfidf_vectorizer = vectorize_with_tfidf(X_train_preprocessed)\n",
    "tfidf_matrix_valid = tfidf_vectorizer.transform(X_valid_preprocessed)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# GloVe-Vektorisierung\n",
    "print(\"Vektorisieren mit GloVe...\")\n",
    "glove_matrix_train = vectorize_with_glove(X_train_preprocessed, glove_embeddings, glove_mean)\n",
    "glove_matrix_valid = vectorize_with_glove(X_valid_preprocessed, glove_embeddings, glove_mean)\n",
    "glove_matrix_test = vectorize_with_glove(X_test_preprocessed, glove_embeddings, glove_mean)\n",
    "\n",
    "# Emoji2Vec-Vektorisierung\n",
    "print(\"Vektorisieren mit Emoji2Vec...\")\n",
    "emoji_matrix_train = vectorize_with_emojis(X_train_preprocessed, emoji_vectors)\n",
    "emoji_matrix_valid = vectorize_with_emojis(X_valid_preprocessed, emoji_vectors)\n",
    "emoji_matrix_test = vectorize_with_emojis(X_test_preprocessed, emoji_vectors)\n",
    "\n",
    "# BERT-Vektorisierung\n",
    "print(\"Vektorisieren mit BERT...\")\n",
    "bert_matrix_train = vectorize_with_bert(X_train, tokenizer, bert_model)\n",
    "bert_matrix_valid = vectorize_with_bert(X_valid, tokenizer, bert_model)\n",
    "bert_matrix_test = vectorize_with_bert(X_test, tokenizer, bert_model)\n",
    "\n",
    "print(\"Feature-Engineering abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modelltraining\n",
    "\n",
    "In diesem Schritt wird ein **LightGBM-Modell** trainiert, das für tabellarische Daten besonders gut geeignet ist.  \n",
    "\n",
    "#### **Einstellungen des Modells**:\n",
    "- **Random State**: Für Reproduzierbarkeit.\n",
    "- **Class Weight**: \"Balanced\", um Ungleichgewichte zwischen Klassen auszugleichen.\n",
    "- **Hyperparameter**:\n",
    "  - `n_estimators=500`: Anzahl der Entscheidungsbäume.\n",
    "  - `max_depth=20`: Maximale Tiefe der Bäume.\n",
    "\n",
    "#### **Ablauf**:\n",
    "1. **Feature-Kombination**:\n",
    "   - TF-IDF, GloVe, Emoji2Vec und BERT-Features werden zu einem einzigen Feature-Set kombiniert.\n",
    "2. **Skalierung**:\n",
    "   - Die kombinierten Features werden standardisiert, um die Verteilung auszugleichen.\n",
    "3. **Training**:\n",
    "   - Das Modell wird mit den skalierten Features trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from train_model import train_lightgbm, evaluate_model, plot_confusion_matrix\n",
    "\n",
    "# Feature-Kombination\n",
    "print(\"Kombiniere Features...\")\n",
    "combined_train = np.hstack([tfidf_matrix_train.toarray(), glove_matrix_train, emoji_matrix_train, bert_matrix_train])\n",
    "combined_valid = np.hstack([tfidf_matrix_valid.toarray(), glove_matrix_valid, emoji_matrix_valid, bert_matrix_valid])\n",
    "combined_test = np.hstack([tfidf_matrix_test.toarray(), glove_matrix_test, emoji_matrix_test, bert_matrix_test])\n",
    "\n",
    "# Skalierung der Features\n",
    "print(\"Skaliere Features...\")\n",
    "scaler = StandardScaler()\n",
    "combined_train_scaled = scaler.fit_transform(combined_train)\n",
    "combined_valid_scaled = scaler.transform(combined_valid)\n",
    "combined_test_scaled = scaler.transform(combined_test)\n",
    "\n",
    "# Modelltraining\n",
    "print(\"Trainiere LightGBM-Modell...\")\n",
    "model = train_lightgbm(combined_train_scaled, y_train)\n",
    "\n",
    "# Modellbewertung\n",
    "print(\"Bewertung des Modells...\")\n",
    "accuracy, conf_matrix, report = evaluate_model(model, combined_test_scaled, y_test)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Modellbewertung\n",
    "\n",
    "Nach dem Training wird das Modell mit den Testdaten bewertet, um die endgültige Leistung zu messen.  \n",
    "Die Bewertung umfasst:\n",
    "\n",
    "1. **Testgenauigkeit (Accuracy)**:\n",
    "   - Gibt an, wie viele der Vorhersagen korrekt sind.\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - **Precision**: Anteil der korrekten positiven Vorhersagen an allen positiven Vorhersagen.\n",
    "   - **Recall**: Anteil der korrekt identifizierten positiven Instanzen an allen tatsächlichen positiven Instanzen.\n",
    "   - **F1-Score**: Harmonisches Mittel aus Precision und Recall.\n",
    "\n",
    "3. **Confusion Matrix**:\n",
    "   - Zeigt die Anzahl der True Positives, True Negatives, False Positives und False Negatives.\n",
    "\n",
    "#### **Ziel**:\n",
    "- Überprüfung, ob die Genauigkeit mindestens **90 %** beträgt.\n",
    "- Analyse der Fehlklassifikationen für potenzielle Verbesserungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bewertungsergebnisse anzeigen\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Confusion Matrix visualisieren\n",
    "plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "# Zusätzliche Analyse (optional)\n",
    "false_positives = (conf_matrix[0][1])\n",
    "false_negatives = (conf_matrix[1][0])\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "print(f\"False Negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Fazit\n",
    "\n",
    "#### **Ergebnisse**\n",
    "- **Genauigkeit**: Das Modell erreichte eine Genauigkeit von **96.40 %** auf den Testdaten, was die ursprüngliche Zielvorgabe von **90 %** übertrifft.\n",
    "- **Starke Leistung**:\n",
    "  - **Precision und Recall** sind für beide Klassen (positive und negative Tweets) ausgeglichen.\n",
    "  - **F1-Score** zeigt, dass das Modell robust und zuverlässig arbeitet.\n",
    "\n",
    "#### **Verwendete Technologien**\n",
    "1. **TF-IDF**: Bewährter Ansatz zur numerischen Repräsentation von Textdaten.\n",
    "2. **GloVe und Emoji2Vec**: Ergänzen sich durch semantische und emoji-spezifische Features.\n",
    "3. **BERT**: Kontextuelle Embeddings verbessern die Modellleistung signifikant.\n",
    "4. **LightGBM**: Effektives Modell für tabellarische Daten, das mit wenig Aufwand optimiert werden kann.\n",
    "\n",
    "#### **Herausforderungen**\n",
    "1. **Feature-Kombination**:\n",
    "   - Die Kombination mehrerer Embedding-Methoden erhöht die Komplexität.\n",
    "2. **Speicherverbrauch**:\n",
    "   - Die Verwendung großer vortrainierter Modelle wie BERT kann speicherintensiv sein.\n",
    "3. **Datenverarbeitung**:\n",
    "   - Die Bereinigung und Verarbeitung der Tweets ist zeitaufwändig.\n",
    "\n",
    "#### **Mögliche Verbesserungen**\n",
    "1. **Hyperparameter-Tuning**:\n",
    "   - Optimierung des LightGBM-Modells, um die Leistung weiter zu steigern.\n",
    "2. **Datenaugmentation**:\n",
    "   - Hinzufügen weiterer Trainingsdaten durch Augmentationstechniken.\n",
    "3. **Experimentieren mit anderen Modellen**:\n",
    "   - Testen von Modellen wie XGBoost oder Deep Learning-Ansätzen.\n",
    "\n",
    "#### **Zusammenfassung**\n",
    "Das Projekt demonstriert, wie moderne NLP-Techniken kombiniert werden können, um ein robustes Modell zur Sentiment-Analyse von Tweets zu erstellen. Die Erzielung einer Genauigkeit von über 96 % zeigt die Stärke des Ansatzes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
